{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothes Classification with Support Vector Machines\n",
    "\n",
    "In this notebook we are going to explore the use of Support Vector Machines (SVM) for image classification. We are going to use a new version of the famous MNIST dataset (the original is a dataset of handwritten digits). The version we are going to use is called Fashion MNIST (https://pravarmahajan.github.io/fashion/) and is a dataset of small images of clothes and accessories.\n",
    "\n",
    "\n",
    "\n",
    "The dataset labels are the following:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Insert your surname, name and ID number\n",
    "\n",
    "Student name: Pujatti Mattia <br>\n",
    "ID: 1232236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the required packages\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load Fashion MNIST dataset\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,offset=16).reshape(len(labels), 784)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix your ID (\"numero di matricola\") and the seed for random generator (as usual you can try different seeds)\n",
    "ID = 1232236\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the Fashion MNIST dataset from the 'data' folder and let's normalize the features so that each value is in [0,1] \n",
    "\n",
    "X, y = load_mnist('data', kind='train')\n",
    "# rescale the data\n",
    "X, y = X / 255., y # original pixel values are between 0 and 255\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into training and test. Make sure that each label is present at least 10 times\n",
    "in training. If it is not, then keep adding permutations to the initial data until this \n",
    "happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random permute the data and split into training and test taking the first 500\n",
    "#data samples as training and the rests as test\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 500\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for plotting a image and printing the corresponding label\n",
    "def plot_input(X_matrix, labels, index):\n",
    "    print(\"INPUT:\")\n",
    "    plt.imshow(\n",
    "        X_matrix[index].reshape(28,28),\n",
    "        cmap          = plt.cm.gray_r,\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(\"LABEL: %i\"%labels[index])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try the plotting function\n",
    "plot_input(X_train,y_train,5)\n",
    "plot_input(X_test,y_test,50)\n",
    "plot_input(X_test,y_test,500)\n",
    "plot_input(X_test,y_test,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 1\n",
    "Use a SVM classifier with cross validation to pick a model. Use a 4-fold cross-validation. Let's start with a Linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SVC\n",
    "from sklearn.svm import SVC\n",
    "#import for Cross-Validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters for linear SVM\n",
    "parameters = {'C': [0.0005, 0.005, 0.05, 0.5, 5, 50, 500]}\n",
    "\n",
    "#run linear SVM\n",
    "svc_lin = SVC(kernel='linear')    \n",
    "GridS = GridSearchCV(estimator=svc_lin,param_grid=parameters,cv=4)\n",
    "GridS.fit(X_train,y_train)\n",
    "\n",
    "print ('RESULTS FOR LINEAR KERNEL','\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(GridS.best_params_,'\\n')\n",
    "\n",
    "print(\"Best model:\")\n",
    "print(GridS.best_estimator_,'\\n')\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(GridS.best_score_,'\\n')\n",
    "\n",
    "#print(\"All scores on the grid:\")\n",
    "#print(pd.DataFrame(GridS.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 2\n",
    "Pick a model for the Polynomial kernel with degree=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for poly with degree 2 kernel\n",
    "parameters = {'C': [0.05, 0.5, 5],'gamma':[0.05,0.5,5.]}\n",
    "\n",
    "#run SVM with poly of degree 2 kernel\n",
    "\n",
    "svc_poly2 = SVC(kernel='poly',degree=2)    \n",
    "GridS_poly2 = GridSearchCV(estimator=svc_poly2,param_grid=parameters,cv=4)\n",
    "GridS_poly2.fit(X_train,y_train)\n",
    "\n",
    "print ('RESULTS FOR POLY DEGREE=2 KERNEL')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(GridS_poly2.best_params_,'\\n')\n",
    "\n",
    "print(\"Best model:\")\n",
    "print(GridS_poly2.best_estimator_,'\\n')\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(GridS_poly2.best_score_,'\\n')\n",
    "\n",
    "#print(\"All scores on the grid:\")\n",
    "#print(pd.DataFrame(GridS_poly2.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 3\n",
    "\n",
    "Now let's try a higher degree for the polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters for poly with higher degree kernel\n",
    "parameters = {'C': [0.05, 0.5, 5],'gamma':[0.05,0.5,5.]}\n",
    "\n",
    "#run SVM with poly of higher degree kernel\n",
    "degree = 3\n",
    "\n",
    "svc_polyn = SVC(kernel='poly',degree=degree)    \n",
    "GridS_polyn = GridSearchCV(estimator=svc_polyn,param_grid=parameters,cv=4)\n",
    "GridS_polyn.fit(X_train,y_train)\n",
    "\n",
    "print ('RESULTS FOR POLY DEGREE=', degree, ' KERNEL')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(GridS_polyn.best_params_,'\\n')\n",
    "\n",
    "print(\"Best model:\")\n",
    "print(GridS_polyn.best_estimator_,'\\n')\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(GridS_polyn.best_score_,'\\n')\n",
    "\n",
    "#print(\"All scores on the grid:\")\n",
    "#print(pd.DataFrame(GridS_polyn.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 4\n",
    "Pick a model for the Radial Basis Function kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for rbf SVM\n",
    "parameters = {'C': [0.5, 5, 50, 500],'gamma':[0.005, 0.05, 0.5,5]}\n",
    "\n",
    "#run SVM with rbf kernel\n",
    "\n",
    "svc_rbf = SVC(kernel='rbf')    \n",
    "GridS_rbf = GridSearchCV(estimator=svc_rbf,param_grid=parameters,cv=4)\n",
    "GridS_rbf.fit(X_train,y_train)\n",
    "\n",
    "print ('RESULTS FOR rbf KERNEL')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(GridS_rbf.best_params_,'\\n')\n",
    "\n",
    "print(\"Best model:\")\n",
    "print(GridS_rbf.best_estimator_,'\\n')\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(GridS_rbf.best_score_,'\\n')\n",
    "\n",
    "#print(\"All scores on the grid:\")\n",
    "#print(pd.DataFrame(GridS_rbf.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO5\n",
    "What do you observe when using RBF and polynomial kernels on this dataset ?\n",
    "\n",
    "The first thing we notice is that the score decrease with the rise of the degree in the case of the polynomial kernel. Tipically, the RBF kernel is the best one to use. In this case, with the use of that particular random seed, we find that the method with linear, quatratic or gaussian kernel are almost equivalent, with similar scores. For this reason, while the linear kernel seems to be the one with the best results, we will keep RBF for the next analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 6\n",
    "Report here the best SVM kernel and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and test error for the best SVM model from CV\n",
    "best_SVM = GridS_rbf.best_estimator_\n",
    "\n",
    "training_error = 1. - best_SVM.score(X_train,y_train)\n",
    "test_error = 1. - best_SVM.score(X_test,y_test)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data\n",
    "Now let's do the same but using more data points for training.\n",
    "\n",
    "\n",
    "Choose a new number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 2000 # TODO number of data points, adjust depending on the capabilities of your PC\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use SVM with parameters obtained from the best model for $m_{training} =  2000$. Since it may take a long time to run, you can decide to just let it run for some time and stop it if it does not complete. If you decide to do this, report it in the TO DO 9 cell below.\n",
    "\n",
    "### TO DO 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get training and test error for the best SVM model from CV\n",
    "\n",
    "best_SVM = GridS.best_estimator_\n",
    "\n",
    "training_error = 1. - best_SVM.score(X_train,y_train)\n",
    "test_error = 1. - best_SVM.score(X_test,y_test)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, let's also use logistic regression (with standard parameters from scikit-learn, i.e. some regularization is included).\n",
    "\n",
    "### TO DO 8 Try first without regularization (use a very large large C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e8,solver='newton-cg',penalty='l2',max_iter=1000,multi_class='auto')\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "prediction_training = logreg.predict(X_train)\n",
    "differences_training = (y_train==prediction_training)\n",
    "training_error = (differences_training==False).sum()/prediction_training.shape[0]\n",
    "\n",
    "prediction_test = logreg.predict(X_test)\n",
    "differences_test = (y_test==prediction_test)\n",
    "test_error = (differences_test==False).sum()/prediction_test.shape[0]\n",
    "\n",
    "print (\"Best logistic regression training error: %f\" % training_error)\n",
    "print (\"Best logistic regression test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 9 Then use also some regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_regu = linear_model.LogisticRegression(C=1,solver='newton-cg',penalty='l2',max_iter=1000,multi_class='auto')\n",
    "logreg_regu.fit(X_train,y_train)\n",
    "\n",
    "prediction_training_regu = logreg_regu.predict(X_train)\n",
    "differences_training_regu = (y_train==prediction_training_regu)\n",
    "training_error_regu = (differences_training_regu==False).sum()/prediction_training_regu.shape[0]\n",
    "\n",
    "prediction_test_regu = logreg_regu.predict(X_test)\n",
    "differences_test_regu = (y_test==prediction_test_regu)\n",
    "test_error_regu = (differences_test_regu==False).sum()/prediction_test_regu.shape[0]\n",
    "\n",
    "\n",
    "print (\"Best regularized logistic regression training error: %f\" % training_error_regu)\n",
    "print (\"Best regularized logistic regression test error: %f\" % test_error_regu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 10\n",
    "Compare and discuss:\n",
    "- the results from SVM with m=500 and with m=2000 training data points. If you stopped the SVM, include such aspect in your comparison.\n",
    "- the results of SVM and of Logistic Regression with and without regularization\n",
    "\n",
    "As we can see, in the case with m=2000 samples we obtain an higher training error (as predictable), respect to the case with m=500, that anyway is more similar to the test error, which remains almost the same between the two sets.\n",
    "With logistic regression, instead, we find a null training error in the case with no regularization, indicating that all the samples have been correctly classified.\n",
    "For what regards the test errors, we find similar values for the best SVM and for the Logistic Regression with and without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 10\n",
    "Plot an item of clothing that is missclassified by logistic regression and correctly classified by SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_prediction = logreg_regu.predict(X)\n",
    "LR_prediction_check = np.array((LR_prediction==y))\n",
    "\n",
    "SVM_prediction = best_SVM.predict(X)\n",
    "SVM_prediction_check = np.array((SVM_prediction==y))\n",
    "\n",
    "def plot_random(lr_pred,svm_pred,lr_check,svm_check):\n",
    "    n = np.random.randint(0,lr_pred.shape[0])\n",
    "    if (not lr_check[n]) and svm_check[n]: \n",
    "        plot_input(X,y,n)\n",
    "        print('Logistic Regression prediction: ',lr_pred[n])\n",
    "        print('SVM prediction: ',svm_pred[n])\n",
    "    else: plot_random(lr_pred,svm_pred,lr_check,svm_check)\n",
    "\n",
    "plot_random(LR_prediction,SVM_prediction,LR_prediction_check,SVM_prediction_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 11\n",
    "Plot the confusion matrix for the SVM classifier and for logistic regression.\n",
    "The confusion matrix has one column for each predicted label and one row for each true label. \n",
    "It shows for each class in the corresponding row how many samples belonging to that class gets each possible output label.\n",
    "Notice that the diagonal contains the correctly classified samples, while the other cells correspond to errors.\n",
    "You can obtain it with the sklearn.metrics.confusion_matrix function (see the documentation).\n",
    "Try also to normalize the confusion matrix by the number of samples in each class in order to measure the accuracy on each single class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for better aligned printing of confusion matrix use floatmode='fixed' (not supported in all versions of Python)\n",
    "np.set_printoptions(precision=2, suppress=True) \n",
    "\n",
    "u, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Labels and frequencies in test set: \", counts)\n",
    "\n",
    "confusion_SVM = skm.confusion_matrix(y,SVM_prediction)\n",
    "print(\"\\n Confusion matrix SVM  \\n \\n\", confusion_SVM)\n",
    "print(\"\\n Confusion matrix SVM (normalized)   \\n \\n\", confusion_SVM /counts[:,None] )\n",
    "\n",
    "confusion_LR = skm.confusion_matrix(y,LR_prediction)\n",
    "print(\"\\n Confusion matrix LR  \\n \\n\", confusion_LR)\n",
    "print(\"\\n Confusion matrix LR (normalized)   \\n \\n\", confusion_LR /counts[:,None] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 12\n",
    "Have a look at the confusion matrices and comment on the obtained accuracies. Why some classes have lower accuracies and others an higher one ? Make some guesses on the possible causes.\n",
    "\n",
    "As can be seen from the diagonal values, most of the samples have been correctly classified by both methods, that also have similar results in term of which clothes are more often recognized. In particular, the labels on which they deviate most are the number 2 (pullovers) and 9(ankle boot), for which the logistic regression is more performing, and number 4(coat) for which, instead, SVM is better.\n",
    "Many classes have lower accuracies than others, probably due to the fact that the drawings are similar, and this make bring confusion to the algorithms. In particular, the labels that are less correctly predicted are the 3(dress) and the 6(shirt) that are usually classified by both algorithms as something else (for example 6 and 0, shirts and t-shirts, are often exchanged)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
