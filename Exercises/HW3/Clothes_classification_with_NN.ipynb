{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothes Classification with Neural Networks\n",
    "\n",
    "In this notebook we are going to explore the Neural Networks for image classification. We are going to use the same dataset of the SVM notebook: Fashion MNIST (https://pravarmahajan.github.io/fashion/), a dataset of small images of clothes and accessories.\n",
    "\n",
    "The dataset labels are the following:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the required packages and check Scikit-learn version\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "print ('scikit-learn version: ', sklearn.__version__)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load Fashion MNIST dataset from disk\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,offset=16).reshape(len(labels), 784)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO \n",
    "Place a seed for the random generator (you can use your \"numero di matricola\"). Try to change the seed to see the impact of the randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1232236\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the MNIST dataset and let's normalize the features so that each value is in [0,1]\n",
    "X, y = load_mnist(\"data\")\n",
    "print(\"Number of samples in the MNIST dataset:\", X.shape[0])\n",
    "# rescale the data\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into training and test. We start with a small training set of 600 samples to reduce computation time. Make sure that each label is present at least 10 times\n",
    "in training frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random permute the data and split into training and test taking the first 600\n",
    "#data samples as training and the rests as test\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 600\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for plotting a image and printing the corresponding label\n",
    "def plot_input(X_matrix, labels, index):\n",
    "    print(\"INPUT:\")\n",
    "    plt.imshow(\n",
    "        X_matrix[index].reshape(28,28),\n",
    "        cmap          = plt.cm.gray_r,\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(\"LABEL: %i\"%labels[index])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try the plotting function\n",
    "plot_input(X_train,y_train,10)\n",
    "plot_input(X_test,y_test,100)\n",
    "plot_input(X_test,y_test,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 1\n",
    "\n",
    "Now use a feed-forward Neural Network for prediction. Use the multi-layer perceptron classifier, with the following parameters: max_iter=300, alpha=1e-4, solver='sgd', tol=1e-4, learning_rate_init=.1, random_state=ID (this last parameter ensures the run is the same even if you run it more than once). The alpha parameter is the regularization term.\n",
    "\n",
    "Then, using the default activation function, pick four or five architectures to consider, with different numbers of hidden layers and different sizes. It is not necessary to create huge neural networks, you can limit to 3 layers and, for each layer, its maximum size can be of 100. Evaluate the architectures you chose using GridSearchCV with cv=5.\n",
    "\n",
    "You can reduce the number of iterations if the running time is too long on your computer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are sample values but feel free to change them as you like, try to experiment with different sizes!!\n",
    "parameters = {'hidden_layer_sizes': [(10,), (20,), (40,), (40,20,), (40,30,20) ]}\n",
    "\n",
    "mlp = MLPClassifier(max_iter=300, alpha=1e-4, solver='sgd',\n",
    "                    tol=1e-4, random_state=ID,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "GridS = GridSearchCV(estimator=mlp,param_grid=parameters,cv=5)\n",
    "GridS.fit(X_train,y_train)\n",
    "\n",
    "print ('RESULTS FOR NN\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(GridS.best_params_,'\\n')\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(GridS.best_score_,'\\n')\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(pd.DataFrame(GridS.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 2\n",
    "\n",
    "Now try also different batch sizes, while keeping the best NN architecture you have found above. Remember that the batch size was previously set to the default value, i.e., min(200, n_samples). \n",
    "Recall that a batch size of 1 corresponds to baseline SGD, while using all the 480 training samples (there are 600 samples but in cross validation with 5 folders we use 1/5 of them for validation at each round) corresponds to standard GD and using a different mini-batch size lies in the middle between the two extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are sample values corresponding to baseline SGD, a reasonable mini-batch size and standard GD\n",
    "# again feel free to change them as you like, try to experiment with different batch sizes!!\n",
    "parameters = {'batch_size': [1, 32, 480]}\n",
    "\n",
    "# need to specify that you would like to use the standard k-fold split otherwise sklearn create splits of different sizes\n",
    "kf = sklearn.model_selection.KFold(n_splits=5)\n",
    "kf.split(X_train,y_train)\n",
    "\n",
    "# recall to use cv=kf in GridSearchCV parameters to use the k-fold subdivision seen in the lectures\n",
    "\n",
    "GridS_kf = GridSearchCV(estimator=GridS.best_estimator_,param_grid=parameters,cv=kf)\n",
    "GridS_kf.fit(X_train,y_train)\n",
    "\n",
    "print ('RESULTS FOR NN\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(GridS_kf.best_params_,'\\n')\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(GridS_kf.best_score_,'\\n')\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(pd.DataFrame(GridS_kf.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "\n",
    "What do you observe for different architectures and batch sizes? How do the number of layers and their sizes affect the performances? What do you observe for different batch sizes, in particular what happens to the training convergence for different batch sizes (notice that the algorithm could not converge for some batch sizes)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using huge neural networks don't pay for this kind of problems, since in the first test the more performant possibilities are the ones with 1 or 2 hidden layers. Using more layers means worse performances. For what regard the batch sizes, instead, we find similar results for the larger values, meaning that using the GD method is the best way. Other possibilities bring several results, but also the risk of no convergence.\n",
    " \n",
    "<br>\n",
    "Anyway, the computation time and the resources required increase a lot with complex NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 3\n",
    "\n",
    "Now try also to use different learning rates, while keeping the best NN architecture and batch size you have found above. Plot the learning curves (i.e., the variation of the loss over the steps, you can get it from the loss_curve_ object of sklearn) for the different values of the learning rate . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "lr_list = [10**exp for exp in range(-3,0)]\n",
    "scores = {}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "bestNN_model = GridS_kf.best_estimator_\n",
    "bestNN_model.set_params(**{'max_iter':1000})\n",
    "\n",
    "kf = sklearn.model_selection.KFold(n_splits=5)\n",
    "kf.split(X_train,y_train)\n",
    "\n",
    "GridS_lr = GridSearchCV(estimator=bestNN_model,param_grid={'learning_rate_init':lr_list},cv=kf)\n",
    "GridS_lr.fit(X_train,y_train)\n",
    "\n",
    "for learning_rate_init in lr_list:\n",
    "    bestNN_model.set_params(**{'learning_rate_init':learning_rate_init})\n",
    "    bestNN_model.fit(X_train,y_train)\n",
    "    label = 'learning_rate ' + str(learning_rate_init)\n",
    "    ax.plot(bestNN_model.loss_curve_,label=label)\n",
    "\n",
    "ax.set_title('Learning curves for different NN architectures')\n",
    "ax.legend()\n",
    "\n",
    "print ('RESULTS FOR NN\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(GridS_lr.best_params_,'\\n')\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(GridS_lr.best_score_,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2\n",
    "\n",
    "Comment about the learning curves (i.e. the variation of the loss over the steps). How does the curve changes for different learning rates in terms of stability and speed of convergence ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before answering to question 2, notice that I have incremented the number of max iterations to 1000, to give to all the models the possibility to reach a better \"visive\" trend in the graph (even if they don't converged yet). The first thing we see looking at the plot is the irregularity of the curve with learning rate equal to 0.1, and the general behaviour of the curves, that tells us that we have the best performances for higher values of the parameter considered. But, even if the model with the best results is the one highest learning rate, for the next computation we will consider the parameter equal to 0.01, to take into account that shows a better stability. Our choice is also confirmed by repeating the analysis with max_iter set to 300: in that case, in fact, the best models is exactly the one with learning rate 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 4\n",
    "\n",
    "Now get training and test error for a NN with best parameters (architecture, batch size and learning rate)from above. Plot the learning curve also for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get training and test error for the best NN model from CV\n",
    "\n",
    "final_bestNN_model = GridS_lr.best_estimator_\n",
    "final_bestNN_model.set_params(**{'learning_rate_init':0.01})  # To have more stability on the learning curve\n",
    "final_bestNN_model.fit(X_train,y_train)\n",
    "\n",
    "training_error = 1. - final_bestNN_model.score(X_train,y_train)\n",
    "test_error = 1. - final_bestNN_model.score(X_test,y_test)\n",
    "\n",
    "print ('\\nRESULTS FOR BEST NN\\n')\n",
    "\n",
    "print (\"Best NN training error: %f\" % training_error)\n",
    "print (\"Best NN test error: %f\" % test_error)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.plot(final_bestNN_model.loss_curve_,label='learning_curve')\n",
    "ax.legend()\n",
    "ax.set_title('Learning curve for the best model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data \n",
    "Now let's do the same but using 5000 (or less if it takes too long on your machine) data points for training. Use the same NN architecture as before, but you can try more if you like and have a powerful computer !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 10000\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 5\n",
    "\n",
    "Now train the NNs with the added data points using the optimum parameters found above. Eventually, feel free to try different architectures if you like. We suggest that you use 'verbose=True' so have an idea of how long it takes to run 1 iteration (eventually reduce also the number of iterations to 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best architecture and params from before\n",
    "final_bestNN_model_moredata = sklearn.base.clone(final_bestNN_model)\n",
    "final_bestNN_model_moredata.set_params(**{'verbose':True})\n",
    "final_bestNN_model_moredata.fit(X_train,y_train)\n",
    "\n",
    "# I KNOW THAT 1000 ITERATIONS IS A BIG NUMBER, BUT THE COMPUTATION STOP AFTER ABOUT 600 ITERATIONS, AND ON MY \n",
    "# PC IT TAKES ONLY 10 SECONDS (IT STARTS TO BECOME MUCH LONGER WITH M_TRAINING = 30000)\n",
    "\n",
    "training_error = 1. - final_bestNN_model_moredata.score(X_train,y_train)\n",
    "test_error = 1. - final_bestNN_model_moredata.score(X_test,y_test)\n",
    "\n",
    "print ('\\nRESULTS FOR NN\\n')\n",
    "\n",
    "print (\"NN training error: %f\" % training_error)\n",
    "print (\"NN test error: %f\" % test_error)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.plot(final_bestNN_model_moredata.loss_curve_,label='learning_curve')\n",
    "ax.legend()\n",
    "ax.set_title('Learning curve for the best model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 3\n",
    "Compare the train and test errors you got with a large number of samples with the best one you obtained with only 600 data points. Comment about the results you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 600 samples, the results weren't so good, in particular in terms of the training error, \n",
    "that was null: probably a case of overfitting. The test error, instead, was quite good, about 0.21. \n",
    "We obtained better performances with bigger dataset. As I said in the previous cell, \n",
    "I tried with m_training values equal to 10000 and 30000, that with so high number of iterations selected, \n",
    "can result into a wasteful algorithm, from a computational point of view. <br>\n",
    "In fact, I obtained the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m_training = 10000 <br>\n",
    "time = 10 seconds <br><br>\n",
    "NN training error: 0.029100 <br>\n",
    "NN test error: 0.166160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m_training = 30000 <br>\n",
    "time = 30/60 seconds <br><br>\n",
    "NN training error: 0.059033 <br>\n",
    "NN test error: 0.145633"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it isn't convenient to use much samples, since the results doesn't change much more, while the computational time is much more bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 7\n",
    "\n",
    "Plot an example that was missclassified by NN with m=600 training data points and it is now instead correctly classified by NN with m=5000 training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_prediction = final_bestNN_model.predict(X)\n",
    "NN_pred_check = np.array((NN_prediction==y))\n",
    "\n",
    "large_NN_prediction = final_bestNN_model_moredata.predict(X)\n",
    "large_NN_pred_check = np.array((large_NN_prediction==y))\n",
    "\n",
    "def plot_random(nn_pred,large_nn_pred,NN_check,large_NN_check):\n",
    "    n = np.random.randint(0,nn_pred.shape[0])\n",
    "    if (not NN_check[n]) and large_NN_check[n]: \n",
    "        plot_input(X,y,n)\n",
    "        print('Neural Network prediction: ',nn_pred[n])\n",
    "        print('Larger Neural Network prediction: ',large_nn_pred[n])\n",
    "    else: plot_random(nn_pred,large_nn_pred,NN_check,large_NN_check)\n",
    "\n",
    "plot_random(NN_prediction,large_NN_prediction,NN_pred_check,large_NN_pred_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 8\n",
    "\n",
    "Let's plot the weigths of the multi-layer perceptron classifier, for the best NN we get with 600 data points and with 5000 data points. The code is already provided, just fix variable names (e.g., replace mlp ,  mlp_large with your estimators) in order to have it working with your implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is already provided, fix variable names in order to have it working with your implementation\n",
    "\n",
    "mlp = final_bestNN_model\n",
    "mlp_large = final_bestNN_model_moredata\n",
    "\n",
    "print(\"Weights with 600 data points:\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 4,figsize=(8,8))\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Weights with 5000 data points:\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 4,figsize=(8,8))\n",
    "vmin, vmax = mlp_large.coefs_[0].min(), mlp_large.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 4\n",
    "\n",
    "Describe what do you observe by looking at the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first difference to notice is that in the second case, the parameters are much more uniformly distributed than in the first one. We observe also less irregularities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 9\n",
    "\n",
    "Report the best SVM model and its parameters, you found in the last notebook (or check out the solution on the moodle webpage of the course). Fit it on a few data points and compute its training and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_training = 5000\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:2*m_training]\n",
    "y_train, y_test = y[:m_training], y[m_training:2*m_training]\n",
    "\n",
    "# use best parameters found in the SVM notebook, create SVM and perform fitting\n",
    "SVM = SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=0.005, kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False) \n",
    "\n",
    "SVM.fit(X_train,y_train)\n",
    "\n",
    "print ('RESULTS FOR SVM')\n",
    "\n",
    "SVM_training_error = 1 - SVM.score(X_train,y_train)\n",
    "\n",
    "print(\"Training score SVM:\")\n",
    "print(SVM_training_error)\n",
    "\n",
    "SVM_test_error = 1 - SVM.score(X_test,y_test)\n",
    "\n",
    "print(\"Test score SVM:\")\n",
    "print(SVM_test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## QUESTION 5\n",
    "Compare the results of SVM and of NN. Which one would you prefer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen just looking at the errors, NN fits better training data, but the performances on the test set are similar between the two methods. But if I had do make a choice I would prefer SVM, since it is computationally less wasteful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
